# 4.1

def analyze_topics(): 

  import nltk
  from nltk.corpus import stopwords
  from nltk.tokenize import word_tokenize
  from nltk.stem import WordNetLemmatizer
  from gensim.corpora import Dictionary
  from gensim.models.ldamodel import LdaModel
  from gensim.models.coherencemodel import CoherenceModel
  from collections import Counter 
  nltk.download('punkt') 
  nltk.download('stopwords') 
  nltk.download('wordnet') 
  
  stop_words = set(stopwords.words('english')) 
  stop_words.extend(['would', 'best', 'always', 'amazing', 'bought', 'quick' 'people', 'new', 'fun', 'think', 'know', 'believe', 'many', 'thing', 'need', 'small', 'even', 'make', 'love', 'mean', 'fact', 'question', 'time', 'reason', 'also', 'could', 'true', 'well',  'life', 'said', 'year', 'going', 'good', 'really', 'much', 'want', 'back', 'look', 'article', 'host', 'university', 'reply', 'thanks', 'mail', 'post', 'please']) 
  lemmatizer = WordNetLemmatizer()
   
  all_posts = query_db("SELECT content FROM posts")
 
  bow_list = []
  for post in all_posts: 
      text = post.get('content', '')  
      tokens = word_tokenize(text.lower()) # get the lower case words 
      tokens = [lemmatizer.lemmatize(t) for t in tokens if t.isalpha() and t not in stop_words] #lemmatise the words and filter out stop words 
      tokens = [t for t in tokens if len(t) > 2]  # filter out words with less than 3 letter s 
      if tokens: 
          bow_list.append(tokens) 
  if not bow_list: 
      print("No posts to analyse") 
      return 
  dictionary = Dictionary(bow_list) 
  dictionary.filter_extremes(no_below=2, no_above=0.3) 
  corpus = [dictionary.doc2bow(tokens) for tokens in bow_list] 
 
  lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10, random_state=2) 
    print("Top 10 Topics:") 
  for i, topic in lda_model.print_topics(num_words=6): 
    print(f"Topic {i}: {topic}")
 
  post_topics = [] 
  for bow in corpus: 
    topic_probs = lda_model.get_document_topics(bow) 
    dominant_topic = max(topic_probs, key=lambda x: x[1])[0] if topic_probs else None 
    post_topics.append(dominant_topic)

# 4.2

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from collections import defaultdict

def analyze_platform_sentiment(lda_model, dictionary, query_db):
 sia = SentimentIntensityAnalyzer()

 posts = query_db("SELECT id, user_id, content FROM posts") 
 comments = query_db("SELECT id, user_id, content FROM comments")
 
 for item in posts + comments:
  text = item.get('content', '') 
  item['sentiment'] = sia.polarity_scores(text)['compound'] 

 all_scores = [item['sentiment'] for item in posts + comments] 
 overall_sentiment = sum(all_scores) / len(all_scores) if all_scores else 0 

 for i, count in enumerate(topic_counts): 
  print(f"Topic {i}: {count} posts")


# 4.4

# Python

def get_reaction_count(post_id): 

 result = query_db("SELECT COUNT(*) as count FROM reactions WHERE post_id = ?", (post_id,), one=True)
 return result['count'] if result else 0

def get_post_badge(reaction_count, created_at): 
 
 now = datetime.utcnow()
 age_days = (now - created_at).days
 if reaction_count >= 10 and age_days <= 7: 
  return "âœ¨ Trending" 
 elif reaction_count >= 50: 
  return "ğŸ”¥ Popular" 
 elif reaction_count >= 25: 
  return "â­ Favourite" 
 elif reaction_count >= 12: 
  return "ğŸ“ˆ Rising" 
 else: 
     return None

# inside def feed():

post = dict(post) 
 
 post_id = post['id'] 
 created_at = post['created_at'] 
 
 reaction_count = get_reaction_count(post_id) 
 badge = get_post_badge(reaction_count, created_at) 
 
 post['badge'] = badge


# HTML

{% if entry.post.badge %}
          <span class="badge bg-warning text-dark ms-2">
              {{ entry.post.badge }}
          </span>
{% endif %} 

 
